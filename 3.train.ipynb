{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbef6d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rilab/ros2_ws/src/deep_learning_proj/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lerobot.common.datasets.factory import make_dataset\n",
    "from lerobot.configs.train import TrainPipelineConfig\n",
    "from lerobot.configs.default import DatasetConfig\n",
    "from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.common.optim.factory import make_optimizer_and_scheduler\n",
    "from lerobot.common.datasets.utils import cycle\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.common.utils.utils import (\n",
    "    format_big_number,\n",
    "    get_safe_torch_device,\n",
    "    has_method,\n",
    "    init_logging,\n",
    ")\n",
    "import torch\n",
    "from src.policies.baseline.configuration import BaselineConfig\n",
    "from src.policies.baseline.modeling import BaselinePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9d3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"./dataset/transformed_data\"\n",
    "CKPT_PATH = \"./ckpt/baseline_model\"\n",
    "LOG_EVERY = 100\n",
    "TOTAL_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92edef6",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "This is the baseline configurations that can be edited. \n",
    "\n",
    "```\n",
    "@PreTrainedConfig.register_subclass(\"omy_baseline\")\n",
    "@dataclass\n",
    "class BaselineConfig(PreTrainedConfig):\n",
    "    # Input / output structure.\n",
    "    n_obs_steps: int = 1\n",
    "    chunk_size: int = 5\n",
    "    n_action_steps: int = 5\n",
    "\n",
    "    # Architecture.\n",
    "    backbone: str = 'mlp' # 'mlp' or 'transformer'\n",
    "    # Vision encoder\n",
    "    vision_backbone: str =\"facebook/dinov3-vitb16-pretrain-lvd1689m\" #\"facebook/dinov2-base\"\n",
    "    projection_dim : int = 128\n",
    "    freeze_backbone:  bool = True\n",
    "\n",
    "\n",
    "    # Num hidden layers\n",
    "    n_hidden_layers: int = 5\n",
    "    hidden_dim: int = 512   \n",
    "\n",
    "    ## For transformer-based architectures\n",
    "    n_heads: int = 4\n",
    "    dim_feedforward: int = 2048\n",
    "    feedforward_activation: str = \"gelu\"\n",
    "    dropout: float = 0.1\n",
    "    pre_norm: bool = True\n",
    "    n_encoder_layers: int = 6\n",
    "\n",
    "    # Training preset\n",
    "    optimizer_lr: float = 1e-3\n",
    "    optimizer_weight_decay: float = 1e-6\n",
    "\n",
    "    # Learning rate scheduler parameters \n",
    "    lr_warmup_steps: int = 1000\n",
    "    total_training_steps: int = 500000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d21f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Device 'None' is not available. Switching to 'cuda'.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load Dataset and Configurations     \n",
    "'''\n",
    "# Dataset Config\n",
    "dataset_cfg = DatasetConfig(\"transformed_data\")\n",
    "dataset_cfg.root = ROOT\n",
    "pipeline_cfg = TrainPipelineConfig(dataset_cfg)\n",
    "\n",
    "# Policy Config\n",
    "cfg = BaselineConfig(\n",
    "    chunk_size=10,\n",
    "    n_action_steps=10,\n",
    "    backbone='mlp',\n",
    "    optimizer_lr= 5e-4,\n",
    "    n_hidden_layers=10,\n",
    "    hidden_dim=512,\n",
    "    # If you are using image features, uncomment the following line\n",
    "    vision_backbone='facebook/dinov3-vitb16-pretrain-lvd1689m',#\"facebook/dinov2-base\", **You need access to use this model** Use dinov2 if you don't have access\n",
    "    projection_dim=128,\n",
    "    freeze_backbone=True,\n",
    "\n",
    ")\n",
    "# Create kwargs and configure pipeline\n",
    "kwargs = {}\n",
    "pipeline_cfg.policy = cfg\n",
    "pipeline_cfg.optimizer = cfg.get_optimizer_preset()\n",
    "pipeline_cfg.scheduler = cfg.get_scheduler_preset()\n",
    "\n",
    "# Create Dataset\n",
    "# Meta data is for loading dataset statistics and feature information\n",
    "dataset = make_dataset(pipeline_cfg)\n",
    "ds_meta = dataset.meta\n",
    "features = dataset_to_policy_features(ds_meta.features)\n",
    "kwargs[\"dataset_stats\"] = ds_meta.stats\n",
    "cfg.output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "cfg.input_features = {key: ft for key, ft in features.items() if key not in cfg.output_features}\n",
    "kwargs[\"config\"] = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7fee07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselinePolicy(\n",
       "  (normalize_inputs): Normalize(\n",
       "    (buffer_observation_state): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "        (std): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (normalize_targets): Normalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "        (std): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (unnormalize_outputs): Unnormalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "        (std): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (model): BaselineModel(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=34, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (9): ReLU()\n",
       "      (10): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (13): ReLU()\n",
       "      (14): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (15): ReLU()\n",
       "      (16): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (17): ReLU()\n",
       "      (18): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (19): ReLU()\n",
       "      (20): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (21): ReLU()\n",
       "      (22): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (23): ReLU()\n",
       "      (24): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (25): ReLU()\n",
       "      (26): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (27): ReLU()\n",
       "      (28): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (29): ReLU()\n",
       "      (30): Linear(in_features=512, out_features=70, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''`\n",
    "Instantiate Policy\n",
    "'''\n",
    "policy = BaselinePolicy(**kwargs)\n",
    "policy.to(pipeline_cfg.policy.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6143b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create Optimizer and Scheduler\n",
    "'''\n",
    "optimizer, lr_scheduler = make_optimizer_and_scheduler(pipeline_cfg, policy)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size= 64,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f8c0f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 4M\n",
      "Number of trainable parameters: 4M\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check Parameter Counts\n",
    "'''\n",
    "trainable_params = [p for p in policy.parameters() if p.requires_grad]\n",
    "total_params = list(policy.parameters())\n",
    "print(f\"Total number of parameters: {format_big_number(sum(p.numel() for p in total_params))}\")\n",
    "print(f\"Number of trainable parameters: {format_big_number(sum(p.numel() for p in trainable_params))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86f20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "Step: 100, Loss: 0.7163, learning rate: 0.000050\n",
      "Starting epoch 2/100\n",
      "Step: 200, Loss: 0.5083, learning rate: 0.000100\n",
      "Step: 300, Loss: 0.4773, learning rate: 0.000150\n",
      "Starting epoch 3/100\n",
      "Step: 400, Loss: 0.3662, learning rate: 0.000200\n",
      "Starting epoch 4/100\n",
      "Step: 500, Loss: 0.3720, learning rate: 0.000250\n",
      "Step: 600, Loss: 0.3381, learning rate: 0.000300\n",
      "Starting epoch 5/100\n",
      "Step: 700, Loss: 0.3574, learning rate: 0.000350\n",
      "Starting epoch 6/100\n",
      "Step: 800, Loss: 0.3616, learning rate: 0.000400\n",
      "Step: 900, Loss: 0.3173, learning rate: 0.000450\n",
      "Starting epoch 7/100\n",
      "Step: 1000, Loss: 0.3062, learning rate: 0.000500\n",
      "Step: 1100, Loss: 0.3142, learning rate: 0.000500\n",
      "Starting epoch 8/100\n",
      "Step: 1200, Loss: 0.2907, learning rate: 0.000500\n",
      "Starting epoch 9/100\n",
      "Step: 1300, Loss: 0.2408, learning rate: 0.000500\n",
      "Step: 1400, Loss: 0.2738, learning rate: 0.000500\n",
      "Starting epoch 10/100\n",
      "Step: 1500, Loss: 0.2406, learning rate: 0.000500\n",
      "Starting epoch 11/100\n",
      "Step: 1600, Loss: 0.2332, learning rate: 0.000500\n",
      "Step: 1700, Loss: 0.2333, learning rate: 0.000500\n",
      "Starting epoch 12/100\n",
      "Step: 1800, Loss: 0.2472, learning rate: 0.000500\n",
      "Step: 1900, Loss: 0.2628, learning rate: 0.000500\n",
      "Starting epoch 13/100\n",
      "Step: 2000, Loss: 0.2345, learning rate: 0.000500\n",
      "Starting epoch 14/100\n",
      "Step: 2100, Loss: 0.2158, learning rate: 0.000500\n",
      "Step: 2200, Loss: 0.2188, learning rate: 0.000500\n",
      "Starting epoch 15/100\n",
      "Step: 2300, Loss: 0.2043, learning rate: 0.000500\n",
      "Starting epoch 16/100\n",
      "Step: 2400, Loss: 0.2182, learning rate: 0.000500\n",
      "Step: 2500, Loss: 0.2226, learning rate: 0.000500\n",
      "Starting epoch 17/100\n",
      "Step: 2600, Loss: 0.2401, learning rate: 0.000500\n",
      "Step: 2700, Loss: 0.2147, learning rate: 0.000500\n",
      "Starting epoch 18/100\n",
      "Step: 2800, Loss: 0.2144, learning rate: 0.000500\n",
      "Starting epoch 19/100\n",
      "Step: 2900, Loss: 0.1927, learning rate: 0.000500\n",
      "Step: 3000, Loss: 0.2046, learning rate: 0.000500\n",
      "Starting epoch 20/100\n",
      "Step: 3100, Loss: 0.2110, learning rate: 0.000500\n",
      "Starting epoch 21/100\n",
      "Step: 3200, Loss: 0.2036, learning rate: 0.000500\n",
      "Step: 3300, Loss: 0.2106, learning rate: 0.000500\n",
      "Starting epoch 22/100\n",
      "Step: 3400, Loss: 0.2097, learning rate: 0.000500\n",
      "Starting epoch 23/100\n",
      "Step: 3500, Loss: 0.1951, learning rate: 0.000500\n",
      "Step: 3600, Loss: 0.1895, learning rate: 0.000500\n",
      "Starting epoch 24/100\n",
      "Step: 3700, Loss: 0.2095, learning rate: 0.000500\n",
      "Step: 3800, Loss: 0.2048, learning rate: 0.000500\n",
      "Starting epoch 25/100\n",
      "Step: 3900, Loss: 0.2118, learning rate: 0.000500\n",
      "Starting epoch 26/100\n",
      "Step: 4000, Loss: 0.2102, learning rate: 0.000500\n",
      "Step: 4100, Loss: 0.1889, learning rate: 0.000500\n",
      "Starting epoch 27/100\n",
      "Step: 4200, Loss: 0.1770, learning rate: 0.000500\n",
      "Starting epoch 28/100\n",
      "Step: 4300, Loss: 0.1733, learning rate: 0.000500\n",
      "Step: 4400, Loss: 0.1999, learning rate: 0.000500\n",
      "Starting epoch 29/100\n",
      "Step: 4500, Loss: 0.1768, learning rate: 0.000500\n",
      "Step: 4600, Loss: 0.1768, learning rate: 0.000500\n",
      "Starting epoch 30/100\n",
      "Step: 4700, Loss: 0.2013, learning rate: 0.000500\n",
      "Starting epoch 31/100\n",
      "Step: 4800, Loss: 0.1919, learning rate: 0.000500\n",
      "Step: 4900, Loss: 0.1898, learning rate: 0.000500\n",
      "Starting epoch 32/100\n",
      "Step: 5000, Loss: 0.1843, learning rate: 0.000500\n",
      "Starting epoch 33/100\n",
      "Step: 5100, Loss: 0.1796, learning rate: 0.000500\n",
      "Step: 5200, Loss: 0.1970, learning rate: 0.000500\n",
      "Starting epoch 34/100\n",
      "Step: 5300, Loss: 0.1652, learning rate: 0.000500\n",
      "Step: 5400, Loss: 0.1948, learning rate: 0.000500\n",
      "Starting epoch 35/100\n",
      "Step: 5500, Loss: 0.1876, learning rate: 0.000500\n",
      "Starting epoch 36/100\n",
      "Step: 5600, Loss: 0.1838, learning rate: 0.000500\n",
      "Step: 5700, Loss: 0.1662, learning rate: 0.000500\n",
      "Starting epoch 37/100\n",
      "Step: 5800, Loss: 0.1983, learning rate: 0.000500\n",
      "Starting epoch 38/100\n",
      "Step: 5900, Loss: 0.1708, learning rate: 0.000500\n",
      "Step: 6000, Loss: 0.1794, learning rate: 0.000500\n",
      "Starting epoch 39/100\n",
      "Step: 6100, Loss: 0.1732, learning rate: 0.000500\n",
      "Step: 6200, Loss: 0.1851, learning rate: 0.000500\n",
      "Starting epoch 40/100\n",
      "Step: 6300, Loss: 0.1847, learning rate: 0.000500\n",
      "Starting epoch 41/100\n",
      "Step: 6400, Loss: 0.1646, learning rate: 0.000500\n",
      "Step: 6500, Loss: 0.1829, learning rate: 0.000500\n",
      "Starting epoch 42/100\n",
      "Step: 6600, Loss: 0.1828, learning rate: 0.000500\n",
      "Starting epoch 43/100\n",
      "Step: 6700, Loss: 0.1710, learning rate: 0.000500\n",
      "Step: 6800, Loss: 0.1682, learning rate: 0.000500\n",
      "Starting epoch 44/100\n",
      "Step: 6900, Loss: 0.1660, learning rate: 0.000500\n",
      "Starting epoch 45/100\n",
      "Step: 7000, Loss: 0.1780, learning rate: 0.000500\n",
      "Step: 7100, Loss: 0.1500, learning rate: 0.000500\n",
      "Starting epoch 46/100\n",
      "Step: 7200, Loss: 0.1481, learning rate: 0.000500\n",
      "Step: 7300, Loss: 0.1678, learning rate: 0.000500\n",
      "Starting epoch 47/100\n",
      "Step: 7400, Loss: 0.1742, learning rate: 0.000500\n",
      "Starting epoch 48/100\n",
      "Step: 7500, Loss: 0.1822, learning rate: 0.000500\n",
      "Step: 7600, Loss: 0.1554, learning rate: 0.000500\n",
      "Starting epoch 49/100\n",
      "Step: 7700, Loss: 0.1804, learning rate: 0.000500\n",
      "Starting epoch 50/100\n",
      "Step: 7800, Loss: 0.1627, learning rate: 0.000500\n",
      "Step: 7900, Loss: 0.1597, learning rate: 0.000500\n",
      "Starting epoch 51/100\n",
      "Step: 8000, Loss: 0.1531, learning rate: 0.000500\n",
      "Step: 8100, Loss: 0.1574, learning rate: 0.000500\n",
      "Starting epoch 52/100\n",
      "Step: 8200, Loss: 0.1756, learning rate: 0.000500\n",
      "Starting epoch 53/100\n",
      "Step: 8300, Loss: 0.1598, learning rate: 0.000500\n",
      "Step: 8400, Loss: 0.1720, learning rate: 0.000500\n",
      "Starting epoch 54/100\n",
      "Step: 8500, Loss: 0.1480, learning rate: 0.000500\n",
      "Starting epoch 55/100\n",
      "Step: 8600, Loss: 0.1467, learning rate: 0.000500\n",
      "Step: 8700, Loss: 0.1604, learning rate: 0.000500\n",
      "Starting epoch 56/100\n",
      "Step: 8800, Loss: 0.1668, learning rate: 0.000500\n",
      "Step: 8900, Loss: 0.1609, learning rate: 0.000500\n",
      "Starting epoch 57/100\n",
      "Step: 9000, Loss: 0.1664, learning rate: 0.000500\n",
      "Starting epoch 58/100\n",
      "Step: 9100, Loss: 0.1622, learning rate: 0.000500\n",
      "Step: 9200, Loss: 0.1652, learning rate: 0.000500\n",
      "Starting epoch 59/100\n",
      "Step: 9300, Loss: 0.1612, learning rate: 0.000500\n",
      "Starting epoch 60/100\n",
      "Step: 9400, Loss: 0.1742, learning rate: 0.000500\n",
      "Step: 9500, Loss: 0.1698, learning rate: 0.000500\n",
      "Starting epoch 61/100\n",
      "Step: 9600, Loss: 0.1626, learning rate: 0.000500\n",
      "Starting epoch 62/100\n",
      "Step: 9700, Loss: 0.1499, learning rate: 0.000500\n",
      "Step: 9800, Loss: 0.1784, learning rate: 0.000500\n",
      "Starting epoch 63/100\n",
      "Step: 9900, Loss: 0.1551, learning rate: 0.000500\n",
      "Step: 10000, Loss: 0.1496, learning rate: 0.000500\n",
      "Starting epoch 64/100\n",
      "Step: 10100, Loss: 0.1561, learning rate: 0.000500\n",
      "Starting epoch 65/100\n",
      "Step: 10200, Loss: 0.1549, learning rate: 0.000499\n",
      "Step: 10300, Loss: 0.1625, learning rate: 0.000499\n",
      "Starting epoch 66/100\n",
      "Step: 10400, Loss: 0.1589, learning rate: 0.000499\n",
      "Starting epoch 67/100\n",
      "Step: 10500, Loss: 0.1734, learning rate: 0.000499\n",
      "Step: 10600, Loss: 0.1431, learning rate: 0.000499\n",
      "Starting epoch 68/100\n",
      "Step: 10700, Loss: 0.1489, learning rate: 0.000499\n",
      "Step: 10800, Loss: 0.1811, learning rate: 0.000499\n",
      "Starting epoch 69/100\n",
      "Step: 10900, Loss: 0.1507, learning rate: 0.000499\n",
      "Starting epoch 70/100\n",
      "Step: 11000, Loss: 0.1549, learning rate: 0.000499\n",
      "Step: 11100, Loss: 0.1349, learning rate: 0.000499\n",
      "Starting epoch 71/100\n",
      "Step: 11200, Loss: 0.1492, learning rate: 0.000499\n",
      "Starting epoch 72/100\n",
      "Step: 11300, Loss: 0.1600, learning rate: 0.000499\n",
      "Step: 11400, Loss: 0.1457, learning rate: 0.000499\n",
      "Starting epoch 73/100\n",
      "Step: 11500, Loss: 0.1502, learning rate: 0.000499\n",
      "Step: 11600, Loss: 0.1747, learning rate: 0.000499\n",
      "Starting epoch 74/100\n",
      "Step: 11700, Loss: 0.1452, learning rate: 0.000499\n",
      "Starting epoch 75/100\n",
      "Step: 11800, Loss: 0.1493, learning rate: 0.000499\n",
      "Step: 11900, Loss: 0.1582, learning rate: 0.000499\n",
      "Starting epoch 76/100\n",
      "Step: 12000, Loss: 0.1490, learning rate: 0.000499\n",
      "Starting epoch 77/100\n",
      "Step: 12100, Loss: 0.1515, learning rate: 0.000499\n",
      "Step: 12200, Loss: 0.1467, learning rate: 0.000499\n",
      "Starting epoch 78/100\n",
      "Step: 12300, Loss: 0.1487, learning rate: 0.000499\n",
      "Step: 12400, Loss: 0.1464, learning rate: 0.000499\n",
      "Starting epoch 79/100\n",
      "Step: 12500, Loss: 0.1607, learning rate: 0.000499\n",
      "Starting epoch 80/100\n",
      "Step: 12600, Loss: 0.1385, learning rate: 0.000499\n",
      "Step: 12700, Loss: 0.1458, learning rate: 0.000499\n",
      "Starting epoch 81/100\n",
      "Step: 12800, Loss: 0.1557, learning rate: 0.000499\n",
      "Starting epoch 82/100\n",
      "Step: 12900, Loss: 0.1513, learning rate: 0.000499\n",
      "Step: 13000, Loss: 0.1562, learning rate: 0.000499\n",
      "Starting epoch 83/100\n",
      "Step: 13100, Loss: 0.1442, learning rate: 0.000499\n",
      "Starting epoch 84/100\n",
      "Step: 13200, Loss: 0.1349, learning rate: 0.000499\n",
      "Step: 13300, Loss: 0.1539, learning rate: 0.000499\n",
      "Starting epoch 85/100\n",
      "Step: 13400, Loss: 0.1434, learning rate: 0.000499\n",
      "Step: 13500, Loss: 0.1425, learning rate: 0.000499\n",
      "Starting epoch 86/100\n",
      "Step: 13600, Loss: 0.1523, learning rate: 0.000499\n",
      "Starting epoch 87/100\n",
      "Step: 13700, Loss: 0.1466, learning rate: 0.000499\n",
      "Step: 13800, Loss: 0.1489, learning rate: 0.000499\n",
      "Starting epoch 88/100\n",
      "Step: 13900, Loss: 0.1506, learning rate: 0.000499\n",
      "Starting epoch 89/100\n",
      "Step: 14000, Loss: 0.1908, learning rate: 0.000499\n",
      "Step: 14100, Loss: 0.1435, learning rate: 0.000499\n",
      "Starting epoch 90/100\n",
      "Step: 14200, Loss: 0.1457, learning rate: 0.000499\n",
      "Step: 14300, Loss: 0.1439, learning rate: 0.000499\n",
      "Starting epoch 91/100\n",
      "Step: 14400, Loss: 0.1431, learning rate: 0.000499\n",
      "Starting epoch 92/100\n",
      "Step: 14500, Loss: 0.1400, learning rate: 0.000499\n",
      "Step: 14600, Loss: 0.1473, learning rate: 0.000499\n",
      "Starting epoch 93/100\n",
      "Step: 14700, Loss: 0.1369, learning rate: 0.000499\n",
      "Starting epoch 94/100\n",
      "Step: 14800, Loss: 0.1646, learning rate: 0.000499\n",
      "Step: 14900, Loss: 0.1370, learning rate: 0.000499\n",
      "Starting epoch 95/100\n",
      "Step: 15000, Loss: 0.1404, learning rate: 0.000499\n",
      "Step: 15100, Loss: 0.1417, learning rate: 0.000499\n",
      "Starting epoch 96/100\n",
      "Step: 15200, Loss: 0.1262, learning rate: 0.000499\n",
      "Starting epoch 97/100\n",
      "Step: 15300, Loss: 0.1438, learning rate: 0.000499\n",
      "Step: 15400, Loss: 0.1470, learning rate: 0.000499\n",
      "Starting epoch 98/100\n",
      "Step: 15500, Loss: 0.1436, learning rate: 0.000499\n",
      "Starting epoch 99/100\n",
      "Step: 15600, Loss: 0.1436, learning rate: 0.000499\n",
      "Step: 15700, Loss: 0.1439, learning rate: 0.000499\n",
      "Starting epoch 100/100\n",
      "Step: 15800, Loss: 0.1340, learning rate: 0.000499\n",
      "Step: 15900, Loss: 0.1433, learning rate: 0.000499\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training Loop\n",
    "'''\n",
    "device = get_safe_torch_device(pipeline_cfg.policy.device, log=True)\n",
    "step = 0\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    print(f\"Starting epoch {epoch+1}/{TOTAL_EPOCHS}\")\n",
    "    for batch in dataloader:\n",
    "        for key in batch:\n",
    "            if isinstance(batch[key], torch.Tensor):\n",
    "                batch[key] = batch[key].to(device, non_blocking=True)\n",
    "        policy.train()\n",
    "        loss, output_dict = policy.forward(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step through pytorch scheduler at every batch instead of epoch\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "        step += 1\n",
    "        if step % LOG_EVERY == 0:\n",
    "            print(f\"Step: {step}, Loss: {loss.item():.4f}, learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "# Save checkpoint at the end of training\n",
    "policy.save_pretrained(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7437858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
